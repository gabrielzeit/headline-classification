{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAN WITH PYTHON 3.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "\n",
    "\n",
    "# if os.path.exists(\"big_merged_dataset.csv\"):\n",
    "#     # Read the CSV file\n",
    "#     merged_df = pd.read_csv(\"big_merged_dataset.csv\")\n",
    "#     headline_data = pd.read_csv('analyst_ratings_processed.csv' , quotechar='\"', quoting=2, delimiter=\",\")\n",
    "#     print(\"CSV file exists. DataFrame loaded successfully.\")\n",
    "# else:\n",
    "#     print(\"CSV file does not exist. Processing starts, this takes approximately 3 minutes\")\n",
    "\n",
    "#     headline_data = pd.read_csv('analyst_ratings_processed.csv' , quotechar='\"', quoting=2, delimiter=\",\")\n",
    "\n",
    "\n",
    "\n",
    "#     # Function to check if the value is a string\n",
    "#     def is_string(value):\n",
    "#         return isinstance(value, str)\n",
    "\n",
    "#     # Filter the DataFrame to keep only rows where the 'date' column is a string\n",
    "#     headline_data = headline_data[headline_data['date'].apply(is_string)]\n",
    "#     print(len(headline_data))\n",
    "\n",
    "\n",
    "\n",
    "#     headline_data[\"date\"] = headline_data[\"date\"].apply(lambda x: x.split(\" \")[0])\n",
    "#     print(\"Date processing done.\")\n",
    "\n",
    "\n",
    "\n",
    "#     # iterable list\n",
    "#     rows = list(headline_data.iterrows())\n",
    "#     print(\"Iterable list transformation done.\")\n",
    "\n",
    "#     merged_df = pd.DataFrame(columns=[\"title\",\"diff_bool\",\"stock\"])\n",
    "#     ticker = \"\"  # save the current stock ticker\n",
    "#     start_block_idx = 0     # save the current stock-block start index to subset dataframe\n",
    "\n",
    "#     # Iterate over the list with access to the next row\n",
    "#     for i in range(len(rows) - 1):\n",
    "#         current_index, current_row = rows[i]\n",
    "#         ticker = current_row[\"stock\"]\n",
    "\n",
    "#         next_index, next_row = rows[i + 1]\n",
    "\n",
    "#         if next_row[\"stock\"] != ticker:\n",
    "#             print(\"Processing stock: \", ticker, \" | progress: \" , (len(merged_df)/len(headline_data)))\n",
    "#             # create subset, merge, insert in plain\n",
    "#             temp_headline_dataset = headline_data.iloc[start_block_idx:current_index]\n",
    "            \n",
    "#             try:\n",
    "#                 raw_stock_data = pd.read_csv(f'stocks/{ticker}.csv') # read the current stock prices\n",
    "#             except:\n",
    "#                 continue\n",
    "#             raw_stock_data = raw_stock_data.rename(columns={'Date': 'date'})\n",
    "\n",
    "#             # Calculate difference as boolean and numeric\n",
    "#             raw_stock_data[\"diff_num\"] = raw_stock_data[\"Close\"] - raw_stock_data[\"Open\"]\n",
    "#             raw_stock_data[\"diff_bool\"] = raw_stock_data[\"diff_num\"].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "#             # Make a small DF and join with headline data\n",
    "#             stock_data = raw_stock_data[[\"date\",\"diff_bool\"]]\n",
    "#             temp_merged_df = pd.merge(temp_headline_dataset, stock_data, on='date', how='inner')\n",
    "#             temp_merged_df = temp_merged_df[[\"title\",\"diff_bool\",\"stock\"]]\n",
    "\n",
    "#             merged_df = pd.concat([merged_df, temp_merged_df], axis=0, ignore_index=True) #merging to the overall dataset\n",
    "\n",
    "#             # setting variables for further iteration of next stock-block\n",
    "#             start_block_idx = next_index\n",
    "#             ticker = next_row[\"stock\"]\n",
    "\n",
    "#     #export the csv so that the computation does not always have to be made\n",
    "#     merged_df.to_csv(\"big_merged_dataset.csv\",index=False)\n",
    "\n",
    "# df = merged_df\n",
    "# print(\"Length is: \" , len(df), \" rows\")\n",
    "# print(\"The dataset holds \", round((len(df)/len(headline_data))*100,2) , \"% of the overall rows.\")\n",
    "# df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>diff_bool</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  diff_bool\n",
       "0  is upset that he can't update his Facebook by ...          0\n",
       "1  @Kenichan I dived many times for the ball. Man...          0\n",
       "2    my whole body feels itchy and like its on fire           0\n",
       "3  @nationwideclass no, it's not behaving at all....          0\n",
       "4                      @Kwesidei not the whole crew           0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"training.1600000.processed.noemoticon.csv\",encoding='latin-1')\n",
    "df = df.iloc[:, [0, 5]]\n",
    "df.columns.values[0] = \"diff_bool\"\n",
    "df.columns.values[1] = \"title\"\n",
    "df[\"diff_bool\"] = df[\"diff_bool\"].replace(4, 1)\n",
    "df = df.loc[:,['title','diff_bool']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>diff_bool</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>120640</th>\n",
       "      <td>i have to write a speech on &amp;quot;speeches&amp;quo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189703</th>\n",
       "      <td>Yeah, this is just great, sick in the middle o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595284</th>\n",
       "      <td>anyway, gotta go.. CSI: NY up next and I have ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021096</th>\n",
       "      <td>@skimhannahkeys @AliciaSkimbit i like your app...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98291</th>\n",
       "      <td>@Lolene ummmmm.... U disappeared... So much fo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     title  diff_bool\n",
       "120640   i have to write a speech on &quot;speeches&quo...          0\n",
       "189703   Yeah, this is just great, sick in the middle o...          0\n",
       "1595284  anyway, gotta go.. CSI: NY up next and I have ...          1\n",
       "1021096  @skimhannahkeys @AliciaSkimbit i like your app...          1\n",
       "98291    @Lolene ummmmm.... U disappeared... So much fo...          0"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for weak computation power, smaple n rows\n",
    "df = df.sample(n=15000, random_state=1)\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/gabriel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/gabriel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>diff_bool</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>120640</th>\n",
       "      <td>write speech quot speeches quot wtf thanks raz</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189703</th>\n",
       "      <td>yeah great sick middle night ca sleep gon na s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595284</th>\n",
       "      <td>anyway got ta go csi ny next asia memorize tyl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021096</th>\n",
       "      <td>skimhannahkeys aliciaskimbit like approach get...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98291</th>\n",
       "      <td>lolene ummmmm u disappeared much nite</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     title  diff_bool\n",
       "120640      write speech quot speeches quot wtf thanks raz          0\n",
       "189703   yeah great sick middle night ca sleep gon na s...          0\n",
       "1595284  anyway got ta go csi ny next asia memorize tyl...          1\n",
       "1021096  skimhannahkeys aliciaskimbit like approach get...          1\n",
       "98291                lolene ummmmm u disappeared much nite          0"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove stopwords an punktuation\n",
    "\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Download the stopwords and punkt tokenizer from NLTK\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Function to clean text by removing stopwords and punctuation\n",
    "def clean_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove punctuation and convert to lowercase\n",
    "    tokens = [word.lower() for word in tokens if word.isalpha()]\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if not word in stop_words]\n",
    "    # Join the tokens back into a string\n",
    "    cleaned_text = ' '.join(filtered_tokens)\n",
    "    return cleaned_text\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "df['title'] = df['title'].apply(clean_text)\n",
    "\n",
    "# View the result\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export as csv to be able to be fed into the tabular dataset\n",
    "df.to_csv(\"train-processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff_bool\n",
      "1    7563\n",
      "0    7437\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(df[\"diff_bool\"].value_counts())\n",
    "dd = pd.Series(df[\"diff_bool\"]).value_counts()\n",
    "sns.barplot(x=np.array(['negative','positive']),y=dd.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Read headline_data\n",
    "# headline_data = pd.read_csv('analyst_ratings_processed.csv' , quotechar='\"', quoting=2, delimiter=\",\")\n",
    "\n",
    "# # subset A stock for development-stage\n",
    "# headline_data = headline_data[headline_data[\"stock\"]==\"A\"]\n",
    "\n",
    "# # Cut only date from datetime value\n",
    "# headline_data[\"date\"] = headline_data[\"date\"].apply(lambda x: x.split(\" \")[0])\n",
    "\n",
    "# # Read Stock Data (only A for development-stage)\n",
    "# raw_stock_data = pd.read_csv('A.csv')\n",
    "\n",
    "# # Preprocessing\n",
    "# raw_stock_data = raw_stock_data.rename(columns={'Date': 'date'})\n",
    "\n",
    "# # Calculate difference as boolean and numeric\n",
    "# raw_stock_data[\"diff_num\"] = raw_stock_data[\"Open\"] - raw_stock_data[\"Close\"]\n",
    "# raw_stock_data[\"diff_bool\"] = raw_stock_data[\"diff_num\"].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# # Add label A for joining\n",
    "# raw_stock_data[\"label\"] = \"A\"\n",
    "\n",
    "# # Make a small DF and join with headline data\n",
    "# stock_data = raw_stock_data[[\"date\",\"label\", \"diff_num\",\"diff_bool\"]]\n",
    "# #print(stock_data[stock_data[\"diff_bool\"]==1])\n",
    "# merged_df = pd.merge(headline_data, stock_data, on='date', how='inner')\n",
    "\n",
    "# # Drop unneccesary columns\n",
    "# #merged_df = merged_df.drop(merged_df.columns[0], axis=1)\n",
    "# # merged_df = merged_df.drop(columns=[\"label_x\",\"label_y\"])\n",
    "# merged_df.head()\n",
    "\n",
    "# #merged_df.to_csv('merged_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Shape the training-dataset\n",
    "\n",
    "# #dataset = pd.read_csv(\"Amerged_df.csv\")\n",
    "\n",
    "# dataset = merged_df[[\"title\",\"diff_bool\"]]\n",
    "# dataset.rename(columns = {'diff_bool':'label'})\n",
    "# print(dataset.head())\n",
    "# dataset.to_csv(\"train-processed.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML book OREILLY Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torchtext==0.6.0 in /home/gabriel/.local/lib/python3.10/site-packages (0.6.0)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from torchtext==0.6.0) (1.16.0)\n",
      "Requirement already satisfied: numpy in /home/gabriel/.local/lib/python3.10/site-packages (from torchtext==0.6.0) (1.26.4)\n",
      "Requirement already satisfied: sentencepiece in /home/gabriel/.local/lib/python3.10/site-packages (from torchtext==0.6.0) (0.2.0)\n",
      "Requirement already satisfied: torch in /home/gabriel/.local/lib/python3.10/site-packages (from torchtext==0.6.0) (2.3.0+rocm6.0)\n",
      "Requirement already satisfied: tqdm in /home/gabriel/.local/lib/python3.10/site-packages (from torchtext==0.6.0) (4.66.4)\n",
      "Requirement already satisfied: requests in /home/gabriel/.local/lib/python3.10/site-packages (from torchtext==0.6.0) (2.32.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/gabriel/.local/lib/python3.10/site-packages (from requests->torchtext==0.6.0) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->torchtext==0.6.0) (2020.6.20)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->torchtext==0.6.0) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->torchtext==0.6.0) (1.26.5)\n",
      "Requirement already satisfied: jinja2 in /home/gabriel/.local/lib/python3.10/site-packages (from torch->torchtext==0.6.0) (3.1.3)\n",
      "Requirement already satisfied: sympy in /usr/lib/python3/dist-packages (from torch->torchtext==0.6.0) (1.9)\n",
      "Requirement already satisfied: pytorch-triton-rocm==2.3.0 in /home/gabriel/.local/lib/python3.10/site-packages (from torch->torchtext==0.6.0) (2.3.0)\n",
      "Requirement already satisfied: networkx in /home/gabriel/.local/lib/python3.10/site-packages (from torch->torchtext==0.6.0) (3.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/gabriel/.local/lib/python3.10/site-packages (from torch->torchtext==0.6.0) (4.11.0)\n",
      "Requirement already satisfied: fsspec in /home/gabriel/.local/lib/python3.10/site-packages (from torch->torchtext==0.6.0) (2024.2.0)\n",
      "Requirement already satisfied: filelock in /home/gabriel/.local/lib/python3.10/site-packages (from torch->torchtext==0.6.0) (3.13.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/gabriel/.local/lib/python3.10/site-packages (from jinja2->torch->torchtext==0.6.0) (2.1.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: spacy in /home/gabriel/.local/lib/python3.10/site-packages (3.7.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/gabriel/.local/lib/python3.10/site-packages (from spacy) (2.32.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/gabriel/.local/lib/python3.10/site-packages (from spacy) (4.66.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/gabriel/.local/lib/python3.10/site-packages (from spacy) (0.9.4)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/gabriel/.local/lib/python3.10/site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/gabriel/.local/lib/python3.10/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/gabriel/.local/lib/python3.10/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/gabriel/.local/lib/python3.10/site-packages (from spacy) (24.0)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /home/gabriel/.local/lib/python3.10/site-packages (from spacy) (8.2.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/gabriel/.local/lib/python3.10/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/gabriel/.local/lib/python3.10/site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/gabriel/.local/lib/python3.10/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/gabriel/.local/lib/python3.10/site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: jinja2 in /home/gabriel/.local/lib/python3.10/site-packages (from spacy) (3.1.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/gabriel/.local/lib/python3.10/site-packages (from spacy) (3.4.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/gabriel/.local/lib/python3.10/site-packages (from spacy) (2.7.1)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from spacy) (59.6.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/gabriel/.local/lib/python3.10/site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/gabriel/.local/lib/python3.10/site-packages (from spacy) (6.4.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/gabriel/.local/lib/python3.10/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /home/gabriel/.local/lib/python3.10/site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: language-data>=1.2 in /home/gabriel/.local/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/gabriel/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /home/gabriel/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.18.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/gabriel/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.11.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/gabriel/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.5)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/gabriel/.local/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/gabriel/.local/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/lib/python3/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.0.3)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /home/gabriel/.local/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/gabriel/.local/lib/python3.10/site-packages (from jinja2->spacy) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /home/gabriel/.local/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torchtext==0.6.0\n",
    "%pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000 1500 1500\n",
      "{'title': ['hoping', 'wonderful', 'day', 'even', 'raining', 'feel', 'like', 'calling', 'best', 'friend', 'tamara', 'love', 'twitter', 'addiction', 'x'], 'label': '1'}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import torch\n",
    "from torchtext import data\n",
    "\n",
    "# Define dataset\n",
    "\n",
    "LABEL = data.LabelField(dtype=torch.long, batch_first=True, sequential=False)\n",
    "TITLE = data.Field(tokenize=\"spacy\", lower=True , tokenizer_language=\"en_core_web_sm\")\n",
    "\n",
    "fields = [(\"id\",None), (\"title\", TITLE) , (\"label\", LABEL)]\n",
    "\n",
    "stockDataset = data.TabularDataset(path=\"train-processed.csv\",\n",
    "                                   format=\"csv\",\n",
    "                                   fields=fields,\n",
    "                                   skip_header=True)\n",
    "\n",
    "\n",
    "(train,test,valid) = stockDataset.split(split_ratio=[0.8, 0.1, 0.1])\n",
    "\n",
    "\n",
    "print( len(train), len(test) , len(valid) )\n",
    "print( vars(train.examples[7])) # label = index????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length title vocabulary: 10002\n",
      "Most common words:  [('good', 675), ('like', 604), ('get', 603), ('go', 583), ('day', 582), ('http', 527), ('love', 516), ('got', 510), ('ca', 490), ('today', 480)]\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary\n",
    "\n",
    "vocab_size = 10000\n",
    "TITLE.build_vocab(train, max_size=vocab_size)\n",
    "LABEL.build_vocab(train)\n",
    "\n",
    "print(\"Length title vocabulary:\", len(TITLE.vocab))\n",
    "print(\"Most common words: \", TITLE.vocab.freqs.most_common(10))\n",
    "# at this point no stopwords removal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data iterator\n",
    "\n",
    "device=\"cuda\"\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(  (train, valid, test),\n",
    "                                                                            batch_size = 32,\n",
    "                                                                            device=device,\n",
    "                                                                            sort_key = lambda x: len(x.title),\n",
    "                                                                            sort_within_batch = False   ) # Überlegen: sort_key weg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelLSTM(\n",
       "  (embedding): Embedding(10000, 200)\n",
       "  (encoder): LSTM(200, 100, num_layers=2, dropout=0.5, bidirectional=True)\n",
       "  (predictor): Linear(in_features=200, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define Model\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\n",
    "class ModelLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size=100, embedding_dim=300, no_layers=1, dropout=0.25):\n",
    "        super(ModelLSTM, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.encoder = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, num_layers=no_layers, dropout=dropout, bidirectional=True)\n",
    "        self.predictor = nn.Linear(hidden_size*2, 2)\n",
    "\n",
    "\n",
    "    def forward(self, seq):\n",
    "        embedded = self.embedding(seq)\n",
    "        output, (hidden, _) = self.encoder(embedded)\n",
    "        preds = self.predictor(hidden[-2:].transpose(0, 1).contiguous().view(-1, hidden.size(2)*2))\n",
    "        return preds\n",
    "\n",
    "# Assuming you have defined vocab_size and device\n",
    "hidden = 100\n",
    "embed = 200 \n",
    "number_layers = 2\n",
    "dropout_rate = 0.5\n",
    "\n",
    "model = ModelLSTM(vocab_size, hidden, embed, number_layers, dropout_rate)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "# class ModelLSTM(nn.Module):\n",
    "#     def __init__(self, hidden_size, embedding_dim, vocab_size):\n",
    "#         super(ModelLSTM, self).__init__()\n",
    "\n",
    "#         self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "#         self.encoder = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, num_layers=1)\n",
    "#         self.predictor = nn.Linear(hidden_size, 2)\n",
    "\n",
    "#     def forward(self, seq):\n",
    "#         output, (hidden,_) = self.encoder(self.embedding(seq))\n",
    "#         preds = self.predictor(hidden.squeeze(0))\n",
    "#         return preds\n",
    "    \n",
    "\n",
    "# model = ModelLSTM(100,300,vocab_size)\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training loop\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# import torch.optim as optim\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# def train(epochs, model, optimizer, criterion, train_iterator, valid_iterator):\n",
    "#     for epoch in range(1, epochs+1):\n",
    "#         training_loss = 0.0\n",
    "#         valid_loss = 0.0\n",
    "#         model.train()\n",
    "\n",
    "#         for batch_idx, batch in enumerate(train_iterator):\n",
    "#             optimizer.zero_grad()\n",
    "#             predict = model(batch.title)\n",
    "#             loss = criterion(predict, batch.label)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             training_loss += loss.data.item() * batch.title.size(0)\n",
    "\n",
    "#         training_loss /= len(train_iterator)\n",
    "#         model.eval()\n",
    "\n",
    "#         for batch_idx, batch in enumerate(valid_iterator):\n",
    "#             predict = model(batch.title)\n",
    "#             loss = criterion(predict, batch.label) \n",
    "#             valid_loss += loss.data.item() * batch.title.size(0)\n",
    "\n",
    "#         valid_loss /= len(valid_iterator)\n",
    "\n",
    "#         print('Epoch: {}, Training Loss: {:.2f}, '\n",
    "#               'Validation Loss: {:.2f}'\n",
    "#               .format(epoch, training_loss, valid_loss))\n",
    "\n",
    "\n",
    "# train(10,model, optimizer,criterion, train_iterator, valid_iterator)\n",
    "\n",
    "# torch.save(model.state_dict(), 'book_model.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import torch.optim as optim\n",
    "# from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# def calculate_accuracy(predict, labels):\n",
    "#     _, preds = torch.max(predict, 1)\n",
    "#     correct = torch.sum(preds == labels).item()\n",
    "#     return correct / len(labels)\n",
    "\n",
    "# def train(epochs, model, optimizer, criterion, train_iterator, valid_iterator):\n",
    "#     for epoch in range(1, epochs+1):\n",
    "#         training_loss = 0.0\n",
    "#         valid_loss = 0.0\n",
    "#         training_correct = 0\n",
    "#         valid_correct = 0\n",
    "\n",
    "#         model.train()\n",
    "#         for batch_idx, batch in enumerate(train_iterator):\n",
    "#             optimizer.zero_grad()\n",
    "#             predict = model(batch.title)\n",
    "#             loss = criterion(predict, batch.label)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             training_loss += loss.item() * batch.title.size(0)\n",
    "#             training_correct += torch.sum(torch.argmax(predict, 1) == batch.label).item()\n",
    "\n",
    "#         training_loss /= len(train_iterator.dataset)\n",
    "#         training_accuracy = training_correct / len(train_iterator.dataset)\n",
    "\n",
    "#         model.eval()\n",
    "#         with torch.no_grad():\n",
    "#             for batch_idx, batch in enumerate(valid_iterator):\n",
    "#                 predict = model(batch.title)\n",
    "#                 loss = criterion(predict, batch.label)\n",
    "#                 valid_loss += loss.item() * batch.title.size(0)\n",
    "#                 valid_correct += torch.sum(torch.argmax(predict, 1) == batch.label).item()\n",
    "\n",
    "#         valid_loss /= len(valid_iterator.dataset)\n",
    "#         valid_accuracy = valid_correct / len(valid_iterator.dataset)\n",
    "\n",
    "#         print('Epoch: {}, Training Loss: {:.2f}, Training Accuracy: {:.2f}, '\n",
    "#               'Validation Loss: {:.2f}, Validation Accuracy: {:.2f}'\n",
    "#               .format(epoch, training_loss, training_accuracy, valid_loss, valid_accuracy))\n",
    "\n",
    "#     torch.save(model.state_dict(), 'book_model.pt')\n",
    "\n",
    "#     # Create confusion matrix\n",
    "#     all_preds = []\n",
    "#     all_labels = []\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         for batch_idx, batch in enumerate(valid_iterator):\n",
    "#             predict = model(batch.title)\n",
    "#             preds = torch.argmax(predict, 1)\n",
    "#             all_preds.extend(preds.cpu().numpy())\n",
    "#             all_labels.extend(batch.label.cpu().numpy())\n",
    "\n",
    "#     cm = confusion_matrix(all_labels, all_preds)\n",
    "#     disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "#     disp.plot()\n",
    "#     plt.show()\n",
    "\n",
    "# train(10, model, optimizer, criterion, train_iterator, valid_iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 0.34, Training Accuracy: 0.57, Validation Loss: 0.16, Validation Accuracy: 0.65\n",
      "Epoch: 2, Training Loss: 0.30, Training Accuracy: 0.67, Validation Loss: 0.15, Validation Accuracy: 0.69\n",
      "Epoch: 3, Training Loss: 0.28, Training Accuracy: 0.72, Validation Loss: 0.15, Validation Accuracy: 0.70\n",
      "Epoch: 4, Training Loss: 0.26, Training Accuracy: 0.75, Validation Loss: 0.15, Validation Accuracy: 0.69\n",
      "Epoch: 5, Training Loss: 0.24, Training Accuracy: 0.77, Validation Loss: 0.14, Validation Accuracy: 0.71\n",
      "Epoch: 6, Training Loss: 0.22, Training Accuracy: 0.80, Validation Loss: 0.15, Validation Accuracy: 0.69\n",
      "Epoch: 7, Training Loss: 0.20, Training Accuracy: 0.83, Validation Loss: 0.16, Validation Accuracy: 0.68\n",
      "Epoch: 8, Training Loss: 0.18, Training Accuracy: 0.85, Validation Loss: 0.16, Validation Accuracy: 0.71\n",
      "Epoch: 9, Training Loss: 0.15, Training Accuracy: 0.87, Validation Loss: 0.17, Validation Accuracy: 0.69\n",
      "Epoch: 10, Training Loss: 0.13, Training Accuracy: 0.90, Validation Loss: 0.18, Validation Accuracy: 0.69\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)  # Added weight decay\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# criterion = nn.BCELoss()\n",
    "\n",
    "def calculate_accuracy(predict, labels):\n",
    "    _, preds = torch.max(predict, 1)\n",
    "    correct = torch.sum(preds == labels).item()\n",
    "    return correct / len(labels)\n",
    "\n",
    "def train(epochs, model, optimizer, criterion, train_iterator, valid_iterator):\n",
    "    best_valid_loss = float('inf')\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        training_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        training_correct = 0\n",
    "        valid_correct = 0\n",
    "\n",
    "        model.train()\n",
    "        for batch_idx, batch in enumerate(train_iterator):\n",
    "            optimizer.zero_grad()\n",
    "            predict = model(batch.title)\n",
    "            loss = criterion(predict, batch.label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_loss += loss.item() * batch.title.size(0)\n",
    "            training_correct += torch.sum(torch.argmax(predict, 1) == batch.label).item()\n",
    "\n",
    "        training_loss /= len(train_iterator.dataset)\n",
    "        training_accuracy = training_correct / len(train_iterator.dataset)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch in enumerate(valid_iterator):\n",
    "                predict = model(batch.title)\n",
    "                loss = criterion(predict, batch.label)\n",
    "                valid_loss += loss.item() * batch.title.size(0)\n",
    "                valid_correct += torch.sum(torch.argmax(predict, 1) == batch.label).item()\n",
    "\n",
    "        valid_loss /= len(valid_iterator.dataset)\n",
    "        valid_accuracy = valid_correct / len(valid_iterator.dataset)\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), 'best_book_model.pt')  # Save best model\n",
    "\n",
    "        print('Epoch: {}, Training Loss: {:.2f}, Training Accuracy: {:.2f}, '\n",
    "              'Validation Loss: {:.2f}, Validation Accuracy: {:.2f}'\n",
    "              .format(epoch, training_loss, training_accuracy, valid_loss, valid_accuracy))\n",
    "\n",
    "    # Create confusion matrix\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(valid_iterator):\n",
    "            predict = model(batch.title)\n",
    "            preds = torch.argmax(predict, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(batch.label.cpu().numpy())\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot()\n",
    "    plt.show()\n",
    "\n",
    "train(10, model, optimizer, criterion, train_iterator, valid_iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model == None:\n",
    "    model.load_state_dict(torch.load('book_model.pt'))\n",
    "    print(\"Model was None -> loaded\")\n",
    "\n",
    "\n",
    "# Classify titles\n",
    "\n",
    "def classify_headline(headline):\n",
    "    categories = {0:\"Stock Down\", 1: \"Stock Up\"}\n",
    "    processed = TITLE.process([TITLE.preprocess(headline)])\n",
    "    processed = processed.to(device)\n",
    "    model.eval()\n",
    "    return categories[model(processed).argmax().item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Stock Up'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_headline(\"KL Gold Reports Record Q1 Gold Sales of 21,014; New Paste Fill Hole 60% Completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualtiy Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Stock Down       0.66      0.64      0.65       720\n",
      "    Stock Up       0.68      0.70      0.69       780\n",
      "\n",
      "    accuracy                           0.67      1500\n",
      "   macro avg       0.67      0.67      0.67      1500\n",
      "weighted avg       0.67      0.67      0.67      1500\n",
      "\n",
      "######\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Stock Down       0.69      0.69      0.69       759\n",
      "    Stock Up       0.68      0.69      0.68       741\n",
      "\n",
      "    accuracy                           0.69      1500\n",
      "   macro avg       0.69      0.69      0.69      1500\n",
      "weighted avg       0.69      0.69      0.69      1500\n",
      "\n",
      "---------------------------------------\n",
      "Accuracy: 0.6720\n",
      "Precision: 0.6714\n",
      "Recall: 0.6709\n",
      "F1-score: 0.6710\n",
      "######\n",
      "Accuracy: 0.6880\n",
      "Precision: 0.6880\n",
      "Recall: 0.6880\n",
      "F1-score: 0.6880\n"
     ]
    }
   ],
   "source": [
    "# with classification_report function\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def evaluate_model(model, data_iterator):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for batch_idx, batch in enumerate(data_iterator):\n",
    "        predictions = model(batch.title)\n",
    "        _, predicted_labels = predictions.max(dim=1)\n",
    "        y_true.extend(batch.label.cpu().numpy())\n",
    "        y_pred.extend(predicted_labels.cpu().numpy())\n",
    "\n",
    "    report = classification_report(y_true, y_pred, target_names=[\"Stock Down\", \"Stock Up\"])\n",
    "    print(report)\n",
    "\n",
    "# Call this method after training to evaluate on test/validation data\n",
    "evaluate_model(model, test_iterator)\n",
    "print(\"######\")\n",
    "evaluate_model(model, valid_iterator)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# calculate extra\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate_model(model, data_iterator):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for batch_idx, batch in enumerate(data_iterator):\n",
    "        predictions = model(batch.title)\n",
    "        _, predicted_labels = predictions.max(dim=1)\n",
    "        y_true.extend(batch.label.cpu().numpy())\n",
    "        y_pred.extend(predicted_labels.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='macro')\n",
    "    recall = recall_score(y_true, y_pred, average='macro')\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "    print(\"Accuracy: {:.4f}\".format(accuracy))\n",
    "    print(\"Precision: {:.4f}\".format(precision))\n",
    "    print(\"Recall: {:.4f}\".format(recall))\n",
    "    print(\"F1-score: {:.4f}\".format(f1))\n",
    "\n",
    "# Call this method after training to evaluate on test/validation data\n",
    "print(\"---------------------------------------\")\n",
    "evaluate_model(model, test_iterator)\n",
    "print(\"######\")\n",
    "evaluate_model(model, valid_iterator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 87\u001b[0m\n\u001b[1;32m     84\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Train and evaluate the model\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m valid_accuracy, model_state \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m completed_combinations \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     90\u001b[0m progress \u001b[38;5;241m=\u001b[39m (completed_combinations \u001b[38;5;241m/\u001b[39m total_combinations) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n",
      "Cell \u001b[0;32mIn[86], line 24\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(epochs, model, optimizer, criterion, train_iterator, valid_iterator)\u001b[0m\n\u001b[1;32m     21\u001b[0m valid_correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     23\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_iterator):\n\u001b[1;32m     25\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     26\u001b[0m     predict \u001b[38;5;241m=\u001b[39m model(batch\u001b[38;5;241m.\u001b[39mtitle)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchtext/data/iterator.py:156\u001b[0m, in \u001b[0;36mIterator.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m             minibatch\u001b[38;5;241m.\u001b[39msort(key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort_key, reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[43mBatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mminibatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepeat:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchtext/data/batch.py:34\u001b[0m, in \u001b[0;36mBatch.__init__\u001b[0;34m(self, data, dataset, device)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m field \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     batch \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mgetattr\u001b[39m(x, name) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[0;32m---> 34\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, \u001b[43mfield\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchtext/data/field.py:237\u001b[0m, in \u001b[0;36mField.process\u001b[0;34m(self, batch, device)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Process a list of examples to create a torch.Tensor.\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \n\u001b[1;32m    228\u001b[0m \u001b[38;5;124;03mPad, numericalize, and postprocess a batch and create a tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;124;03m    and custom postprocessing Pipeline.\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    236\u001b[0m padded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad(batch)\n\u001b[0;32m--> 237\u001b[0m tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumericalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpadded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tensor\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchtext/data/field.py:359\u001b[0m, in \u001b[0;36mField.numericalize\u001b[0;34m(self, arr, device)\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocessing \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    357\u001b[0m         arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocessing(arr, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 359\u001b[0m var \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msequential \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first:\n\u001b[1;32m    362\u001b[0m     var\u001b[38;5;241m.\u001b[39mt_()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_accuracy(predict, labels):\n",
    "    _, preds = torch.max(predict, 1)\n",
    "    correct = torch.sum(preds == labels).item()\n",
    "    return correct / len(labels)\n",
    "\n",
    "def train_and_evaluate(epochs, model, optimizer, criterion, train_iterator, valid_iterator):\n",
    "    best_valid_loss = float('inf')\n",
    "    best_valid_accuracy = 0.0\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        training_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        training_correct = 0\n",
    "        valid_correct = 0\n",
    "\n",
    "        model.train()\n",
    "        for batch_idx, batch in enumerate(train_iterator):\n",
    "            optimizer.zero_grad()\n",
    "            predict = model(batch.title)\n",
    "            loss = criterion(predict, batch.label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_loss += loss.item() * batch.title.size(0)\n",
    "            training_correct += torch.sum(torch.argmax(predict, 1) == batch.label).item()\n",
    "\n",
    "        training_loss /= len(train_iterator.dataset)\n",
    "        training_accuracy = training_correct / len(train_iterator.dataset)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch in enumerate(valid_iterator):\n",
    "                predict = model(batch.title)\n",
    "                loss = criterion(predict, batch.label)\n",
    "                valid_loss += loss.item() * batch.title.size(0)\n",
    "                valid_correct += torch.sum(torch.argmax(predict, 1) == batch.label).item()\n",
    "\n",
    "        valid_loss /= len(valid_iterator.dataset)\n",
    "        valid_accuracy = valid_correct / len(valid_iterator.dataset)\n",
    "\n",
    "        if valid_accuracy > best_valid_accuracy:\n",
    "            best_valid_accuracy = valid_accuracy\n",
    "            best_model_state = model.state_dict()\n",
    "\n",
    "        #print(f'Epoch: {epoch}, Training Loss: {training_loss:.2f}, Training Accuracy: {training_accuracy:.2f}, ' f'Validation Loss: {valid_loss:.2f}, Validation Accuracy: {valid_accuracy:.2f}')\n",
    "\n",
    "    return best_valid_accuracy, best_model_state\n",
    "\n",
    "\n",
    "device=\"cuda\"\n",
    "\n",
    "# Hyperparameters to tune\n",
    "learning_rates = [0.00001, 0.001, 0.01]\n",
    "weight_decay_list = [1e-5, 1e-3]\n",
    "\n",
    "hidden_sizes = [50, 100, 150]\n",
    "embedding_dims = [75, 125, 200]\n",
    "number_layers=[2,5,10]\n",
    "dropout_rates=[0.2, 0.5]\n",
    "\n",
    "epochs_list = [5, 10]\n",
    "\n",
    "# Placeholder for best model and its hyperparameters\n",
    "best_hyperparams = None\n",
    "best_accuracy = 0.0\n",
    "best_model_state = None\n",
    "\n",
    "total_combinations = len(learning_rates) * len(hidden_sizes) * len(embedding_dims) * len(epochs_list) * len(weight_decay_list) * len(number_layers) * len(dropout_rates)\n",
    "completed_combinations = 0\n",
    "\n",
    "# Iterate through all combinations of hyperparameters\n",
    "for lr, hidden_size, embedding_dim, epochs, wd, no_layers, dropout in itertools.product(learning_rates, hidden_sizes, embedding_dims, epochs_list, weight_decay_list, number_layers, dropout_rates):\n",
    "    # Initialize the model with the current set of hyperparameters\n",
    "    model = ModelLSTM(vocab_size, hidden_size=hidden_size, embedding_dim=embedding_dim, no_layers=no_layers, dropout=dropout)  # Adjust model initialization as per your model definition\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Train and evaluate the model\n",
    "    valid_accuracy, model_state = train_and_evaluate(epochs, model, optimizer, criterion, train_iterator, valid_iterator)\n",
    "\n",
    "    completed_combinations += 1\n",
    "    progress = (completed_combinations / total_combinations) * 100\n",
    "    print(f'Progress: {progress:.2f}%')\n",
    "\n",
    "    # Check if the current model is the best one\n",
    "    if valid_accuracy > best_accuracy:\n",
    "        best_accuracy = valid_accuracy\n",
    "        best_hyperparams = (lr, hidden_size, embedding_dim, epochs, wd, no_layers, dropout)\n",
    "        best_model_state = model_state\n",
    "        torch.save(best_model_state, 'best_book_model.pt')\n",
    "        print(f'New best model found with accuracy: {best_accuracy:.2f}')\n",
    "        print(f'Hyperparameters - Learning Rate: {lr}, Hidden Size: {hidden_size}, Embedding Dim: {embedding_dim}, Epochs: {epochs}, Weight Decay: {wd}, Number Layers: {no_layers}, Dropout_rate: {dropout}'), \n",
    "\n",
    "# Final best model details\n",
    "print('Best Validation Accuracy:', best_accuracy)\n",
    "print('Best Hyperparameters:', best_hyperparams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import itertools\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torchtext import data\n",
    "\n",
    "\n",
    "\n",
    "# # Hyperparameters to tune\n",
    "# learning_rates = [0.00001, 0.001, 0.01]\n",
    "# hidden_sizes = [50, 100, 150]\n",
    "# embedding_dims = [200, 300, 400]\n",
    "# epochs = [5, 10]\n",
    "# weight_decay=[1e-5, 1e-3]\n",
    "\n",
    "# # Initialize best validation loss to infinity\n",
    "# best_valid_loss = float('inf')\n",
    "\n",
    "# # Grid Search\n",
    "# for lr, hidden_size, embedding_dim, num_epochs in itertools.product(learning_rates, hidden_sizes, embedding_dims, epochs):\n",
    "    \n",
    "#     # Initialize the model with current set of hyperparameters\n",
    "#     model = ModelLSTM(vocab_size, hidden, embed, number_layers, dropout_rate)\n",
    "#     model.to(device)\n",
    "    \n",
    "#     # Define optimizer and loss function\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "#     # Training and validation loop\n",
    "#     for epoch in range(num_epochs):\n",
    "#         model.train()\n",
    "#         epoch_loss = 0\n",
    "#         for batch in train_iterator:\n",
    "#             optimizer.zero_grad()\n",
    "#             predictions = model(batch.title)\n",
    "#             loss = criterion(predictions, batch.label)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             epoch_loss += loss.item()\n",
    "        \n",
    "#         # Validation\n",
    "#         model.eval()\n",
    "#         valid_loss = 0\n",
    "#         with torch.no_grad():\n",
    "#             for batch in valid_iterator:\n",
    "#                 predictions = model(batch.title)\n",
    "#                 loss = criterion(predictions, batch.label)\n",
    "#                 valid_loss += loss.item()\n",
    "                \n",
    "#         # Average validation loss for this epoch\n",
    "#         valid_loss /= len(valid_iterator)\n",
    "        \n",
    "#         # Print progress\n",
    "#         print(f'Epoch: {epoch+1}, LR: {lr}, Hidden: {hidden_size}, Embed: {embedding_dim}, Valid Loss: {valid_loss}')\n",
    "        \n",
    "#         # Update best model if validation loss improves\n",
    "#         if valid_loss < best_valid_loss:\n",
    "#             best_valid_loss = valid_loss\n",
    "#             torch.save(model.state_dict(), 'best_model.pt')\n",
    "#             print(f\"Best model saved with LR: {lr}, Hidden: {hidden_size}, Embed: {embedding_dim}, Valid Loss: {valid_loss}\")\n",
    "\n",
    "# # Load and use the best model\n",
    "# # best_model = ModelLSTM(vocab_size, best_hidden_size, best_embedding_dim)\n",
    "# # best_model.load_state_dict(torch.load('best_model.pt'))\n",
    "# # best_model.to(device)\n",
    "\n",
    "# # Further code for evaluation and usage of best_model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
