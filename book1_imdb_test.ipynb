{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAN WITH PYTHON 3.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "\n",
    "\n",
    "# if os.path.exists(\"big_merged_dataset.csv\"):\n",
    "#     # Read the CSV file\n",
    "#     merged_df = pd.read_csv(\"big_merged_dataset.csv\")\n",
    "#     headline_data = pd.read_csv('analyst_ratings_processed.csv' , quotechar='\"', quoting=2, delimiter=\",\")\n",
    "#     print(\"CSV file exists. DataFrame loaded successfully.\")\n",
    "# else:\n",
    "#     print(\"CSV file does not exist. Processing starts, this takes approximately 3 minutes\")\n",
    "\n",
    "#     headline_data = pd.read_csv('analyst_ratings_processed.csv' , quotechar='\"', quoting=2, delimiter=\",\")\n",
    "\n",
    "\n",
    "\n",
    "#     # Function to check if the value is a string\n",
    "#     def is_string(value):\n",
    "#         return isinstance(value, str)\n",
    "\n",
    "#     # Filter the DataFrame to keep only rows where the 'date' column is a string\n",
    "#     headline_data = headline_data[headline_data['date'].apply(is_string)]\n",
    "#     print(len(headline_data))\n",
    "\n",
    "\n",
    "\n",
    "#     headline_data[\"date\"] = headline_data[\"date\"].apply(lambda x: x.split(\" \")[0])\n",
    "#     print(\"Date processing done.\")\n",
    "\n",
    "\n",
    "\n",
    "#     # iterable list\n",
    "#     rows = list(headline_data.iterrows())\n",
    "#     print(\"Iterable list transformation done.\")\n",
    "\n",
    "#     merged_df = pd.DataFrame(columns=[\"title\",\"diff_bool\",\"stock\"])\n",
    "#     ticker = \"\"  # save the current stock ticker\n",
    "#     start_block_idx = 0     # save the current stock-block start index to subset dataframe\n",
    "\n",
    "#     # Iterate over the list with access to the next row\n",
    "#     for i in range(len(rows) - 1):\n",
    "#         current_index, current_row = rows[i]\n",
    "#         ticker = current_row[\"stock\"]\n",
    "\n",
    "#         next_index, next_row = rows[i + 1]\n",
    "\n",
    "#         if next_row[\"stock\"] != ticker:\n",
    "#             print(\"Processing stock: \", ticker, \" | progress: \" , (len(merged_df)/len(headline_data)))\n",
    "#             # create subset, merge, insert in plain\n",
    "#             temp_headline_dataset = headline_data.iloc[start_block_idx:current_index]\n",
    "            \n",
    "#             try:\n",
    "#                 raw_stock_data = pd.read_csv(f'stocks/{ticker}.csv') # read the current stock prices\n",
    "#             except:\n",
    "#                 continue\n",
    "#             raw_stock_data = raw_stock_data.rename(columns={'Date': 'date'})\n",
    "\n",
    "#             # Calculate difference as boolean and numeric\n",
    "#             raw_stock_data[\"diff_num\"] = raw_stock_data[\"Close\"] - raw_stock_data[\"Open\"]\n",
    "#             raw_stock_data[\"diff_bool\"] = raw_stock_data[\"diff_num\"].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "#             # Make a small DF and join with headline data\n",
    "#             stock_data = raw_stock_data[[\"date\",\"diff_bool\"]]\n",
    "#             temp_merged_df = pd.merge(temp_headline_dataset, stock_data, on='date', how='inner')\n",
    "#             temp_merged_df = temp_merged_df[[\"title\",\"diff_bool\",\"stock\"]]\n",
    "\n",
    "#             merged_df = pd.concat([merged_df, temp_merged_df], axis=0, ignore_index=True) #merging to the overall dataset\n",
    "\n",
    "#             # setting variables for further iteration of next stock-block\n",
    "#             start_block_idx = next_index\n",
    "#             ticker = next_row[\"stock\"]\n",
    "\n",
    "#     #export the csv so that the computation does not always have to be made\n",
    "#     merged_df.to_csv(\"big_merged_dataset.csv\",index=False)\n",
    "\n",
    "# df = merged_df\n",
    "# print(\"Length is: \" , len(df), \" rows\")\n",
    "# print(\"The dataset holds \", round((len(df)/len(headline_data))*100,2) , \"% of the overall rows.\")\n",
    "# df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>diff_bool</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  diff_bool\n",
       "0  One of the other reviewers has mentioned that ...          1\n",
       "1  A wonderful little production. <br /><br />The...          1\n",
       "2  I thought this was a wonderful way to spend ti...          1\n",
       "3  Basically there's a family where a little boy ...          0\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...          1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"IMDB Dataset.csv\")\n",
    "df = df.rename(columns={\"sentiment\":\"diff_bool\", \"review\":\"title\"})\n",
    "df[\"diff_bool\"] = df[\"diff_bool\"].apply(lambda x: 1 if x==\"positive\" else 0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>diff_bool</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26247</th>\n",
       "      <td>With No Dead Heroes you get stupid lines like ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35067</th>\n",
       "      <td>I thought maybe... maybe this could be good. A...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34590</th>\n",
       "      <td>An elite American military team which of cours...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16668</th>\n",
       "      <td>Ridiculous horror film about a wealthy man (Jo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12196</th>\n",
       "      <td>Well, if you are one of those Katana's film-nu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  diff_bool\n",
       "26247  With No Dead Heroes you get stupid lines like ...          0\n",
       "35067  I thought maybe... maybe this could be good. A...          0\n",
       "34590  An elite American military team which of cours...          0\n",
       "16668  Ridiculous horror film about a wealthy man (Jo...          0\n",
       "12196  Well, if you are one of those Katana's film-nu...          1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for weak computation power, smaple n rows\n",
    "df = df.sample(n=15000, random_state=1)\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/gabriel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/gabriel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>diff_bool</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26247</th>\n",
       "      <td>dead heroes get stupid lines like woefully aby...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35067</th>\n",
       "      <td>thought maybe maybe could good early appearanc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34590</th>\n",
       "      <td>elite american military team course happens in...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16668</th>\n",
       "      <td>ridiculous horror film wealthy man john carrad...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12196</th>\n",
       "      <td>well one katana like sure appreciate metaphysi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  diff_bool\n",
       "26247  dead heroes get stupid lines like woefully aby...          0\n",
       "35067  thought maybe maybe could good early appearanc...          0\n",
       "34590  elite american military team course happens in...          0\n",
       "16668  ridiculous horror film wealthy man john carrad...          0\n",
       "12196  well one katana like sure appreciate metaphysi...          1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove stopwords an punktuation\n",
    "\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Download the stopwords and punkt tokenizer from NLTK\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Function to clean text by removing stopwords and punctuation\n",
    "def clean_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove punctuation and convert to lowercase\n",
    "    tokens = [word.lower() for word in tokens if word.isalpha()]\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if not word in stop_words]\n",
    "    # Join the tokens back into a string\n",
    "    cleaned_text = ' '.join(filtered_tokens)\n",
    "    return cleaned_text\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "df['title'] = df['title'].apply(clean_text)\n",
    "\n",
    "# View the result\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export as csv to be able to be fed into the tabular dataset\n",
    "df.to_csv(\"train-processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff_bool\n",
      "0    7521\n",
      "1    7479\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(df[\"diff_bool\"].value_counts())\n",
    "dd = pd.Series(df[\"diff_bool\"]).value_counts()\n",
    "sns.barplot(x=np.array(['negative','positive']),y=dd.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Read headline_data\n",
    "# headline_data = pd.read_csv('analyst_ratings_processed.csv' , quotechar='\"', quoting=2, delimiter=\",\")\n",
    "\n",
    "# # subset A stock for development-stage\n",
    "# headline_data = headline_data[headline_data[\"stock\"]==\"A\"]\n",
    "\n",
    "# # Cut only date from datetime value\n",
    "# headline_data[\"date\"] = headline_data[\"date\"].apply(lambda x: x.split(\" \")[0])\n",
    "\n",
    "# # Read Stock Data (only A for development-stage)\n",
    "# raw_stock_data = pd.read_csv('A.csv')\n",
    "\n",
    "# # Preprocessing\n",
    "# raw_stock_data = raw_stock_data.rename(columns={'Date': 'date'})\n",
    "\n",
    "# # Calculate difference as boolean and numeric\n",
    "# raw_stock_data[\"diff_num\"] = raw_stock_data[\"Open\"] - raw_stock_data[\"Close\"]\n",
    "# raw_stock_data[\"diff_bool\"] = raw_stock_data[\"diff_num\"].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# # Add label A for joining\n",
    "# raw_stock_data[\"label\"] = \"A\"\n",
    "\n",
    "# # Make a small DF and join with headline data\n",
    "# stock_data = raw_stock_data[[\"date\",\"label\", \"diff_num\",\"diff_bool\"]]\n",
    "# #print(stock_data[stock_data[\"diff_bool\"]==1])\n",
    "# merged_df = pd.merge(headline_data, stock_data, on='date', how='inner')\n",
    "\n",
    "# # Drop unneccesary columns\n",
    "# #merged_df = merged_df.drop(merged_df.columns[0], axis=1)\n",
    "# # merged_df = merged_df.drop(columns=[\"label_x\",\"label_y\"])\n",
    "# merged_df.head()\n",
    "\n",
    "# #merged_df.to_csv('merged_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Shape the training-dataset\n",
    "\n",
    "# #dataset = pd.read_csv(\"Amerged_df.csv\")\n",
    "\n",
    "# dataset = merged_df[[\"title\",\"diff_bool\"]]\n",
    "# dataset.rename(columns = {'diff_bool':'label'})\n",
    "# print(dataset.head())\n",
    "# dataset.to_csv(\"train-processed.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML book OREILLY Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torchtext==0.6.0 in /home/gabriel/.local/lib/python3.10/site-packages (0.6.0)\n",
      "Requirement already satisfied: tqdm in /home/gabriel/.local/lib/python3.10/site-packages (from torchtext==0.6.0) (4.66.4)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from torchtext==0.6.0) (1.16.0)\n",
      "Requirement already satisfied: sentencepiece in /home/gabriel/.local/lib/python3.10/site-packages (from torchtext==0.6.0) (0.2.0)\n",
      "Requirement already satisfied: requests in /home/gabriel/.local/lib/python3.10/site-packages (from torchtext==0.6.0) (2.32.1)\n",
      "Requirement already satisfied: torch in /home/gabriel/.local/lib/python3.10/site-packages (from torchtext==0.6.0) (2.3.0+rocm6.0)\n",
      "Requirement already satisfied: numpy in /home/gabriel/.local/lib/python3.10/site-packages (from torchtext==0.6.0) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->torchtext==0.6.0) (2020.6.20)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->torchtext==0.6.0) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->torchtext==0.6.0) (1.26.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/gabriel/.local/lib/python3.10/site-packages (from requests->torchtext==0.6.0) (3.3.2)\n",
      "Requirement already satisfied: networkx in /home/gabriel/.local/lib/python3.10/site-packages (from torch->torchtext==0.6.0) (3.2.1)\n",
      "Requirement already satisfied: filelock in /home/gabriel/.local/lib/python3.10/site-packages (from torch->torchtext==0.6.0) (3.13.1)\n",
      "Requirement already satisfied: fsspec in /home/gabriel/.local/lib/python3.10/site-packages (from torch->torchtext==0.6.0) (2024.2.0)\n",
      "Requirement already satisfied: pytorch-triton-rocm==2.3.0 in /home/gabriel/.local/lib/python3.10/site-packages (from torch->torchtext==0.6.0) (2.3.0)\n",
      "Requirement already satisfied: jinja2 in /home/gabriel/.local/lib/python3.10/site-packages (from torch->torchtext==0.6.0) (3.1.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/gabriel/.local/lib/python3.10/site-packages (from torch->torchtext==0.6.0) (4.11.0)\n",
      "Requirement already satisfied: sympy in /usr/lib/python3/dist-packages (from torch->torchtext==0.6.0) (1.9)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/gabriel/.local/lib/python3.10/site-packages (from jinja2->torch->torchtext==0.6.0) (2.1.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: spacy in /home/gabriel/.local/lib/python3.10/site-packages (3.7.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/gabriel/.local/lib/python3.10/site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/gabriel/.local/lib/python3.10/site-packages (from spacy) (2.32.1)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/gabriel/.local/lib/python3.10/site-packages (from spacy) (0.9.4)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from spacy) (59.6.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/gabriel/.local/lib/python3.10/site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /home/gabriel/.local/lib/python3.10/site-packages (from spacy) (8.2.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/gabriel/.local/lib/python3.10/site-packages (from spacy) (3.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/gabriel/.local/lib/python3.10/site-packages (from spacy) (24.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/gabriel/.local/lib/python3.10/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/gabriel/.local/lib/python3.10/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/gabriel/.local/lib/python3.10/site-packages (from spacy) (4.66.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/gabriel/.local/lib/python3.10/site-packages (from spacy) (6.4.0)\n",
      "Requirement already satisfied: jinja2 in /home/gabriel/.local/lib/python3.10/site-packages (from spacy) (3.1.3)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /home/gabriel/.local/lib/python3.10/site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/gabriel/.local/lib/python3.10/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/gabriel/.local/lib/python3.10/site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/gabriel/.local/lib/python3.10/site-packages (from spacy) (2.7.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/gabriel/.local/lib/python3.10/site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/gabriel/.local/lib/python3.10/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/gabriel/.local/lib/python3.10/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: language-data>=1.2 in /home/gabriel/.local/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/gabriel/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/gabriel/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.11.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /home/gabriel/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.18.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/gabriel/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.5)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/gabriel/.local/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/gabriel/.local/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/lib/python3/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.0.3)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /home/gabriel/.local/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/gabriel/.local/lib/python3.10/site-packages (from jinja2->spacy) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /home/gabriel/.local/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torchtext==0.6.0\n",
    "%pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000 1500 1500\n",
      "{'title': ['well', 'say', 'nymphomania', 'leaves', 'unsatisfied', 'know', 'stella', 'james', 'sean', 'young', 'qualifies', 'clinical', 'nymphomaniac', 'certainly', 'sexual', 'relations', 'men', 'still', 'exploring', 'trying', 'find', 'data', 'see', 'wants', 'life', 'men', 'though', 'seems', 'like', 'age', 'pretty', 'good', 'idea', 'ca', 'agree', 'however', 'anyone', 'says', 'young', 'old', 'role', 'age', 'nicely', 'part', 'film', 'left', 'cold', 'though', 'means', 'worst', 'type', 'ever', 'see', 'unlike', 'recent', 'wide', 'shut', 'least', 'something', 'happens', 'one'], 'label': '0'}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import torch\n",
    "from torchtext import data\n",
    "\n",
    "# Define dataset\n",
    "\n",
    "LABEL = data.LabelField(dtype=torch.long, batch_first=True, sequential=False)\n",
    "TITLE = data.Field(tokenize=\"spacy\", lower=True , tokenizer_language=\"en_core_web_sm\")\n",
    "\n",
    "fields = [(\"id\",None), (\"title\", TITLE) , (\"label\", LABEL)]\n",
    "\n",
    "stockDataset = data.TabularDataset(path=\"train-processed.csv\",\n",
    "                                   format=\"csv\",\n",
    "                                   fields=fields,\n",
    "                                   skip_header=True)\n",
    "\n",
    "\n",
    "(train,test,valid) = stockDataset.split(split_ratio=[0.8, 0.1, 0.1])\n",
    "\n",
    "\n",
    "print( len(train), len(test) , len(valid) )\n",
    "print( vars(train.examples[7])) # label = index????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length title vocabulary: 10002\n",
      "Most common words:  [('br', 47330), ('movie', 20363), ('film', 18093), ('one', 12523), ('like', 9181), ('good', 6889), ('would', 6198), ('even', 5848), ('time', 5650), ('really', 5573)]\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary\n",
    "\n",
    "vocab_size = 10000\n",
    "TITLE.build_vocab(train, max_size=vocab_size)\n",
    "LABEL.build_vocab(train)\n",
    "\n",
    "print(\"Length title vocabulary:\", len(TITLE.vocab))\n",
    "print(\"Most common words: \", TITLE.vocab.freqs.most_common(10))\n",
    "# at this point no stopwords removal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data iterator\n",
    "\n",
    "device=\"cuda\"\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(  (train, valid, test),\n",
    "                                                                            batch_size = 32,\n",
    "                                                                            device=device,\n",
    "                                                                            sort_key = lambda x: len(x.title),\n",
    "                                                                            sort_within_batch = False   ) # Überlegen: sort_key weg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelLSTM(\n",
       "  (embedding): Embedding(10000, 200)\n",
       "  (encoder): LSTM(200, 100, num_layers=2, dropout=0.5, bidirectional=True)\n",
       "  (predictor): Linear(in_features=200, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define Model\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\n",
    "class ModelLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size=100, embedding_dim=300, no_layers=1, dropout=0.25):\n",
    "        super(ModelLSTM, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.encoder = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, num_layers=no_layers, dropout=dropout, bidirectional=True)\n",
    "        self.predictor = nn.Linear(hidden_size*2, 2)\n",
    "\n",
    "\n",
    "    def forward(self, seq):\n",
    "        embedded = self.embedding(seq)\n",
    "        output, (hidden, _) = self.encoder(embedded)\n",
    "        preds = self.predictor(hidden[-2:].transpose(0, 1).contiguous().view(-1, hidden.size(2)*2))\n",
    "        return preds\n",
    "\n",
    "# Assuming you have defined vocab_size and device\n",
    "hidden = 100\n",
    "embed = 200 \n",
    "number_layers = 2\n",
    "dropout_rate = 0.5\n",
    "\n",
    "model = ModelLSTM(vocab_size, hidden, embed, number_layers, dropout_rate)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "# class ModelLSTM(nn.Module):\n",
    "#     def __init__(self, hidden_size, embedding_dim, vocab_size):\n",
    "#         super(ModelLSTM, self).__init__()\n",
    "\n",
    "#         self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "#         self.encoder = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, num_layers=1)\n",
    "#         self.predictor = nn.Linear(hidden_size, 2)\n",
    "\n",
    "#     def forward(self, seq):\n",
    "#         output, (hidden,_) = self.encoder(self.embedding(seq))\n",
    "#         preds = self.predictor(hidden.squeeze(0))\n",
    "#         return preds\n",
    "    \n",
    "\n",
    "# model = ModelLSTM(100,300,vocab_size)\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training loop\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# import torch.optim as optim\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# def train(epochs, model, optimizer, criterion, train_iterator, valid_iterator):\n",
    "#     for epoch in range(1, epochs+1):\n",
    "#         training_loss = 0.0\n",
    "#         valid_loss = 0.0\n",
    "#         model.train()\n",
    "\n",
    "#         for batch_idx, batch in enumerate(train_iterator):\n",
    "#             optimizer.zero_grad()\n",
    "#             predict = model(batch.title)\n",
    "#             loss = criterion(predict, batch.label)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             training_loss += loss.data.item() * batch.title.size(0)\n",
    "\n",
    "#         training_loss /= len(train_iterator)\n",
    "#         model.eval()\n",
    "\n",
    "#         for batch_idx, batch in enumerate(valid_iterator):\n",
    "#             predict = model(batch.title)\n",
    "#             loss = criterion(predict, batch.label) \n",
    "#             valid_loss += loss.data.item() * batch.title.size(0)\n",
    "\n",
    "#         valid_loss /= len(valid_iterator)\n",
    "\n",
    "#         print('Epoch: {}, Training Loss: {:.2f}, '\n",
    "#               'Validation Loss: {:.2f}'\n",
    "#               .format(epoch, training_loss, valid_loss))\n",
    "\n",
    "\n",
    "# train(10,model, optimizer,criterion, train_iterator, valid_iterator)\n",
    "\n",
    "# torch.save(model.state_dict(), 'book_model.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import torch.optim as optim\n",
    "# from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# def calculate_accuracy(predict, labels):\n",
    "#     _, preds = torch.max(predict, 1)\n",
    "#     correct = torch.sum(preds == labels).item()\n",
    "#     return correct / len(labels)\n",
    "\n",
    "# def train(epochs, model, optimizer, criterion, train_iterator, valid_iterator):\n",
    "#     for epoch in range(1, epochs+1):\n",
    "#         training_loss = 0.0\n",
    "#         valid_loss = 0.0\n",
    "#         training_correct = 0\n",
    "#         valid_correct = 0\n",
    "\n",
    "#         model.train()\n",
    "#         for batch_idx, batch in enumerate(train_iterator):\n",
    "#             optimizer.zero_grad()\n",
    "#             predict = model(batch.title)\n",
    "#             loss = criterion(predict, batch.label)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             training_loss += loss.item() * batch.title.size(0)\n",
    "#             training_correct += torch.sum(torch.argmax(predict, 1) == batch.label).item()\n",
    "\n",
    "#         training_loss /= len(train_iterator.dataset)\n",
    "#         training_accuracy = training_correct / len(train_iterator.dataset)\n",
    "\n",
    "#         model.eval()\n",
    "#         with torch.no_grad():\n",
    "#             for batch_idx, batch in enumerate(valid_iterator):\n",
    "#                 predict = model(batch.title)\n",
    "#                 loss = criterion(predict, batch.label)\n",
    "#                 valid_loss += loss.item() * batch.title.size(0)\n",
    "#                 valid_correct += torch.sum(torch.argmax(predict, 1) == batch.label).item()\n",
    "\n",
    "#         valid_loss /= len(valid_iterator.dataset)\n",
    "#         valid_accuracy = valid_correct / len(valid_iterator.dataset)\n",
    "\n",
    "#         print('Epoch: {}, Training Loss: {:.2f}, Training Accuracy: {:.2f}, '\n",
    "#               'Validation Loss: {:.2f}, Validation Accuracy: {:.2f}'\n",
    "#               .format(epoch, training_loss, training_accuracy, valid_loss, valid_accuracy))\n",
    "\n",
    "#     torch.save(model.state_dict(), 'book_model.pt')\n",
    "\n",
    "#     # Create confusion matrix\n",
    "#     all_preds = []\n",
    "#     all_labels = []\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         for batch_idx, batch in enumerate(valid_iterator):\n",
    "#             predict = model(batch.title)\n",
    "#             preds = torch.argmax(predict, 1)\n",
    "#             all_preds.extend(preds.cpu().numpy())\n",
    "#             all_labels.extend(batch.label.cpu().numpy())\n",
    "\n",
    "#     cm = confusion_matrix(all_labels, all_preds)\n",
    "#     disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "#     disp.plot()\n",
    "#     plt.show()\n",
    "\n",
    "# train(10, model, optimizer, criterion, train_iterator, valid_iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 8.40, Training Accuracy: 0.55, Validation Loss: 2.47, Validation Accuracy: 0.64\n",
      "Epoch: 2, Training Loss: 6.42, Training Accuracy: 0.74, Validation Loss: 1.83, Validation Accuracy: 0.78\n",
      "Epoch: 3, Training Loss: 5.35, Training Accuracy: 0.81, Validation Loss: 1.48, Validation Accuracy: 0.83\n",
      "Epoch: 4, Training Loss: 4.70, Training Accuracy: 0.83, Validation Loss: 1.43, Validation Accuracy: 0.83\n",
      "Epoch: 5, Training Loss: 3.96, Training Accuracy: 0.87, Validation Loss: 1.46, Validation Accuracy: 0.84\n",
      "Epoch: 6, Training Loss: 3.68, Training Accuracy: 0.88, Validation Loss: 1.54, Validation Accuracy: 0.84\n",
      "Epoch: 7, Training Loss: 3.27, Training Accuracy: 0.90, Validation Loss: 1.55, Validation Accuracy: 0.86\n",
      "Epoch: 8, Training Loss: 2.85, Training Accuracy: 0.92, Validation Loss: 1.60, Validation Accuracy: 0.85\n",
      "Epoch: 9, Training Loss: 2.60, Training Accuracy: 0.92, Validation Loss: 1.47, Validation Accuracy: 0.86\n",
      "Epoch: 10, Training Loss: 2.19, Training Accuracy: 0.94, Validation Loss: 1.39, Validation Accuracy: 0.87\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)  # Added weight decay\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# criterion = nn.BCELoss()\n",
    "\n",
    "def calculate_accuracy(predict, labels):\n",
    "    _, preds = torch.max(predict, 1)\n",
    "    correct = torch.sum(preds == labels).item()\n",
    "    return correct / len(labels)\n",
    "\n",
    "def train(epochs, model, optimizer, criterion, train_iterator, valid_iterator):\n",
    "    best_valid_loss = float('inf')\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        training_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        training_correct = 0\n",
    "        valid_correct = 0\n",
    "\n",
    "        model.train()\n",
    "        for batch_idx, batch in enumerate(train_iterator):\n",
    "            optimizer.zero_grad()\n",
    "            predict = model(batch.title)\n",
    "            loss = criterion(predict, batch.label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_loss += loss.item() * batch.title.size(0)\n",
    "            training_correct += torch.sum(torch.argmax(predict, 1) == batch.label).item()\n",
    "\n",
    "        training_loss /= len(train_iterator.dataset)\n",
    "        training_accuracy = training_correct / len(train_iterator.dataset)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch in enumerate(valid_iterator):\n",
    "                predict = model(batch.title)\n",
    "                loss = criterion(predict, batch.label)\n",
    "                valid_loss += loss.item() * batch.title.size(0)\n",
    "                valid_correct += torch.sum(torch.argmax(predict, 1) == batch.label).item()\n",
    "\n",
    "        valid_loss /= len(valid_iterator.dataset)\n",
    "        valid_accuracy = valid_correct / len(valid_iterator.dataset)\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), 'best_book_model.pt')  # Save best model\n",
    "\n",
    "        print('Epoch: {}, Training Loss: {:.2f}, Training Accuracy: {:.2f}, '\n",
    "              'Validation Loss: {:.2f}, Validation Accuracy: {:.2f}'\n",
    "              .format(epoch, training_loss, training_accuracy, valid_loss, valid_accuracy))\n",
    "\n",
    "    # Create confusion matrix\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(valid_iterator):\n",
    "            predict = model(batch.title)\n",
    "            preds = torch.argmax(predict, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(batch.label.cpu().numpy())\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot()\n",
    "    plt.show()\n",
    "\n",
    "train(10, model, optimizer, criterion, train_iterator, valid_iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model == None:\n",
    "    model.load_state_dict(torch.load('book_model.pt'))\n",
    "    print(\"Model was None -> loaded\")\n",
    "\n",
    "\n",
    "# Classify titles\n",
    "\n",
    "def classify_headline(headline):\n",
    "    categories = {0:\"Stock Down\", 1: \"Stock Up\"}\n",
    "    processed = TITLE.process([TITLE.preprocess(headline)])\n",
    "    processed = processed.to(device)\n",
    "    model.eval()\n",
    "    return categories[model(processed).argmax().item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Stock Up'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_headline(\"KL Gold Reports Record Q1 Gold Sales of 21,014; New Paste Fill Hole 60% Completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualtiy Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Stock Down       0.88      0.80      0.84       732\n",
      "    Stock Up       0.83      0.89      0.86       768\n",
      "\n",
      "    accuracy                           0.85      1500\n",
      "   macro avg       0.85      0.85      0.85      1500\n",
      "weighted avg       0.85      0.85      0.85      1500\n",
      "\n",
      "######\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Stock Down       0.88      0.83      0.86       721\n",
      "    Stock Up       0.85      0.90      0.88       779\n",
      "\n",
      "    accuracy                           0.87      1500\n",
      "   macro avg       0.87      0.87      0.87      1500\n",
      "weighted avg       0.87      0.87      0.87      1500\n",
      "\n",
      "---------------------------------------\n",
      "Accuracy: 0.8487\n",
      "Precision: 0.8511\n",
      "Recall: 0.8476\n",
      "F1-score: 0.8481\n",
      "######\n",
      "Accuracy: 0.8667\n",
      "Precision: 0.8683\n",
      "Recall: 0.8653\n",
      "F1-score: 0.8660\n"
     ]
    }
   ],
   "source": [
    "# with classification_report function\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def evaluate_model(model, data_iterator):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for batch_idx, batch in enumerate(data_iterator):\n",
    "        predictions = model(batch.title)\n",
    "        _, predicted_labels = predictions.max(dim=1)\n",
    "        y_true.extend(batch.label.cpu().numpy())\n",
    "        y_pred.extend(predicted_labels.cpu().numpy())\n",
    "\n",
    "    report = classification_report(y_true, y_pred, target_names=[\"Stock Down\", \"Stock Up\"])\n",
    "    print(report)\n",
    "\n",
    "# Call this method after training to evaluate on test/validation data\n",
    "evaluate_model(model, test_iterator)\n",
    "print(\"######\")\n",
    "evaluate_model(model, valid_iterator)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# calculate extra\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate_model(model, data_iterator):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for batch_idx, batch in enumerate(data_iterator):\n",
    "        predictions = model(batch.title)\n",
    "        _, predicted_labels = predictions.max(dim=1)\n",
    "        y_true.extend(batch.label.cpu().numpy())\n",
    "        y_pred.extend(predicted_labels.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='macro')\n",
    "    recall = recall_score(y_true, y_pred, average='macro')\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "    print(\"Accuracy: {:.4f}\".format(accuracy))\n",
    "    print(\"Precision: {:.4f}\".format(precision))\n",
    "    print(\"Recall: {:.4f}\".format(recall))\n",
    "    print(\"F1-score: {:.4f}\".format(f1))\n",
    "\n",
    "# Call this method after training to evaluate on test/validation data\n",
    "print(\"---------------------------------------\")\n",
    "evaluate_model(model, test_iterator)\n",
    "print(\"######\")\n",
    "evaluate_model(model, valid_iterator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 87\u001b[0m\n\u001b[1;32m     84\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Train and evaluate the model\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m valid_accuracy, model_state \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m completed_combinations \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     90\u001b[0m progress \u001b[38;5;241m=\u001b[39m (completed_combinations \u001b[38;5;241m/\u001b[39m total_combinations) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n",
      "Cell \u001b[0;32mIn[21], line 28\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(epochs, model, optimizer, criterion, train_iterator, valid_iterator)\u001b[0m\n\u001b[1;32m     26\u001b[0m predict \u001b[38;5;241m=\u001b[39m model(batch\u001b[38;5;241m.\u001b[39mtitle)\n\u001b[1;32m     27\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(predict, batch\u001b[38;5;241m.\u001b[39mlabel)\n\u001b[0;32m---> 28\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     30\u001b[0m training_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m batch\u001b[38;5;241m.\u001b[39mtitle\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_accuracy(predict, labels):\n",
    "    _, preds = torch.max(predict, 1)\n",
    "    correct = torch.sum(preds == labels).item()\n",
    "    return correct / len(labels)\n",
    "\n",
    "def train_and_evaluate(epochs, model, optimizer, criterion, train_iterator, valid_iterator):\n",
    "    best_valid_loss = float('inf')\n",
    "    best_valid_accuracy = 0.0\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        training_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        training_correct = 0\n",
    "        valid_correct = 0\n",
    "\n",
    "        model.train()\n",
    "        for batch_idx, batch in enumerate(train_iterator):\n",
    "            optimizer.zero_grad()\n",
    "            predict = model(batch.title)\n",
    "            loss = criterion(predict, batch.label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_loss += loss.item() * batch.title.size(0)\n",
    "            training_correct += torch.sum(torch.argmax(predict, 1) == batch.label).item()\n",
    "\n",
    "        training_loss /= len(train_iterator.dataset)\n",
    "        training_accuracy = training_correct / len(train_iterator.dataset)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch in enumerate(valid_iterator):\n",
    "                predict = model(batch.title)\n",
    "                loss = criterion(predict, batch.label)\n",
    "                valid_loss += loss.item() * batch.title.size(0)\n",
    "                valid_correct += torch.sum(torch.argmax(predict, 1) == batch.label).item()\n",
    "\n",
    "        valid_loss /= len(valid_iterator.dataset)\n",
    "        valid_accuracy = valid_correct / len(valid_iterator.dataset)\n",
    "\n",
    "        if valid_accuracy > best_valid_accuracy:\n",
    "            best_valid_accuracy = valid_accuracy\n",
    "            best_model_state = model.state_dict()\n",
    "\n",
    "        #print(f'Epoch: {epoch}, Training Loss: {training_loss:.2f}, Training Accuracy: {training_accuracy:.2f}, ' f'Validation Loss: {valid_loss:.2f}, Validation Accuracy: {valid_accuracy:.2f}')\n",
    "\n",
    "    return best_valid_accuracy, best_model_state\n",
    "\n",
    "\n",
    "device=\"cuda\"\n",
    "\n",
    "# Hyperparameters to tune\n",
    "learning_rates = [0.00001, 0.001, 0.01]\n",
    "weight_decay_list = [1e-5, 1e-3]\n",
    "\n",
    "hidden_sizes = [50, 100, 150]\n",
    "embedding_dims = [75, 125, 200]\n",
    "number_layers=[2,5,10]\n",
    "dropout_rates=[0.2, 0.5]\n",
    "\n",
    "epochs_list = [5, 10]\n",
    "\n",
    "# Placeholder for best model and its hyperparameters\n",
    "best_hyperparams = None\n",
    "best_accuracy = 0.0\n",
    "best_model_state = None\n",
    "\n",
    "total_combinations = len(learning_rates) * len(hidden_sizes) * len(embedding_dims) * len(epochs_list) * len(weight_decay_list) * len(number_layers) * len(dropout_rates)\n",
    "completed_combinations = 0\n",
    "\n",
    "# Iterate through all combinations of hyperparameters\n",
    "for lr, hidden_size, embedding_dim, epochs, wd, no_layers, dropout in itertools.product(learning_rates, hidden_sizes, embedding_dims, epochs_list, weight_decay_list, number_layers, dropout_rates):\n",
    "    # Initialize the model with the current set of hyperparameters\n",
    "    model = ModelLSTM(vocab_size, hidden_size=hidden_size, embedding_dim=embedding_dim, no_layers=no_layers, dropout=dropout)  # Adjust model initialization as per your model definition\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Train and evaluate the model\n",
    "    valid_accuracy, model_state = train_and_evaluate(epochs, model, optimizer, criterion, train_iterator, valid_iterator)\n",
    "\n",
    "    completed_combinations += 1\n",
    "    progress = (completed_combinations / total_combinations) * 100\n",
    "    print(f'Progress: {progress:.2f}%')\n",
    "\n",
    "    # Check if the current model is the best one\n",
    "    if valid_accuracy > best_accuracy:\n",
    "        best_accuracy = valid_accuracy\n",
    "        best_hyperparams = (lr, hidden_size, embedding_dim, epochs, wd, no_layers, dropout)\n",
    "        best_model_state = model_state\n",
    "        torch.save(best_model_state, 'best_book_model.pt')\n",
    "        print(f'New best model found with accuracy: {best_accuracy:.2f}')\n",
    "        print(f'Hyperparameters - Learning Rate: {lr}, Hidden Size: {hidden_size}, Embedding Dim: {embedding_dim}, Epochs: {epochs}, Weight Decay: {wd}, Number Layers: {no_layers}, Dropout_rate: {dropout}'), \n",
    "\n",
    "# Final best model details\n",
    "print('Best Validation Accuracy:', best_accuracy)\n",
    "print('Best Hyperparameters:', best_hyperparams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import itertools\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torchtext import data\n",
    "\n",
    "\n",
    "\n",
    "# # Hyperparameters to tune\n",
    "# learning_rates = [0.00001, 0.001, 0.01]\n",
    "# hidden_sizes = [50, 100, 150]\n",
    "# embedding_dims = [200, 300, 400]\n",
    "# epochs = [5, 10]\n",
    "# weight_decay=[1e-5, 1e-3]\n",
    "\n",
    "# # Initialize best validation loss to infinity\n",
    "# best_valid_loss = float('inf')\n",
    "\n",
    "# # Grid Search\n",
    "# for lr, hidden_size, embedding_dim, num_epochs in itertools.product(learning_rates, hidden_sizes, embedding_dims, epochs):\n",
    "    \n",
    "#     # Initialize the model with current set of hyperparameters\n",
    "#     model = ModelLSTM(vocab_size, hidden, embed, number_layers, dropout_rate)\n",
    "#     model.to(device)\n",
    "    \n",
    "#     # Define optimizer and loss function\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "#     # Training and validation loop\n",
    "#     for epoch in range(num_epochs):\n",
    "#         model.train()\n",
    "#         epoch_loss = 0\n",
    "#         for batch in train_iterator:\n",
    "#             optimizer.zero_grad()\n",
    "#             predictions = model(batch.title)\n",
    "#             loss = criterion(predictions, batch.label)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             epoch_loss += loss.item()\n",
    "        \n",
    "#         # Validation\n",
    "#         model.eval()\n",
    "#         valid_loss = 0\n",
    "#         with torch.no_grad():\n",
    "#             for batch in valid_iterator:\n",
    "#                 predictions = model(batch.title)\n",
    "#                 loss = criterion(predictions, batch.label)\n",
    "#                 valid_loss += loss.item()\n",
    "                \n",
    "#         # Average validation loss for this epoch\n",
    "#         valid_loss /= len(valid_iterator)\n",
    "        \n",
    "#         # Print progress\n",
    "#         print(f'Epoch: {epoch+1}, LR: {lr}, Hidden: {hidden_size}, Embed: {embedding_dim}, Valid Loss: {valid_loss}')\n",
    "        \n",
    "#         # Update best model if validation loss improves\n",
    "#         if valid_loss < best_valid_loss:\n",
    "#             best_valid_loss = valid_loss\n",
    "#             torch.save(model.state_dict(), 'best_model.pt')\n",
    "#             print(f\"Best model saved with LR: {lr}, Hidden: {hidden_size}, Embed: {embedding_dim}, Valid Loss: {valid_loss}\")\n",
    "\n",
    "# # Load and use the best model\n",
    "# # best_model = ModelLSTM(vocab_size, best_hidden_size, best_embedding_dim)\n",
    "# # best_model.load_state_dict(torch.load('best_model.pt'))\n",
    "# # best_model.to(device)\n",
    "\n",
    "# # Further code for evaluation and usage of best_model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
